{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:09:45.423438Z",
     "iopub.status.busy": "2025-08-28T06:09:45.423094Z",
     "iopub.status.idle": "2025-08-28T06:09:45.487565Z",
     "shell.execute_reply": "2025-08-28T06:09:45.486499Z",
     "shell.execute_reply.started": "2025-08-28T06:09:45.423411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import os, math, json, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L, Model\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, balanced_accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "IMG_SIZE = (224, 224)     # match ImageNet pretrain for MobileNetV2/ResNet/EffNet\n",
    "BATCH = 32                # adjust if you see OOM; 16‚Äì32 is fine\n",
    "EPOCHS_HEAD = 8           # was 6\n",
    "EPOCHS_FINE = 20          # was 15\n",
    "\n",
    "# keep ‚Äúsame number per class‚Äù on VAL/TEST, but let TRAIN keep more signal:\n",
    "BALANCE_MODE_TRAIN = \"over\"      # <-- IMPORTANT: oversample train to max per class\n",
    "BALANCE_MODE_VALTEST = \"down\"    # keep val/test strictly equal-sized per class\n",
    "N_PER_CLASS_TRAIN = None         # None => auto (min for down, max for over)\n",
    "N_PER_CLASS_VALTEST = None\n",
    "\n",
    "MODELS_TO_RUN = [\"mobilenetv2\", \"resnet50\", \"efficientnetb0\", \"densenet121\", \"vit_tiny\"]\n",
    "OUTPUT_DIR = \"./outputs_ham10000\"; os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HAM10000 metadata & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:11:55.704323Z",
     "iopub.status.busy": "2025-08-28T06:11:55.703487Z",
     "iopub.status.idle": "2025-08-28T06:11:55.766253Z",
     "shell.execute_reply": "2025-08-28T06:11:55.765193Z",
     "shell.execute_reply.started": "2025-08-28T06:11:55.704290Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw label counts:\n",
      " label\n",
      "akiec     327\n",
      "bcc       514\n",
      "bkl      1099\n",
      "df        115\n",
      "mel      1113\n",
      "nv       6705\n",
      "vasc      142\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Kaggle paths\n",
    "meta_csv = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n",
    "image_dir1 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\"\n",
    "image_dir2 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n",
    "\n",
    "df = pd.read_csv(meta_csv)\n",
    "all_image_paths = {os.path.splitext(f)[0]: os.path.join(image_dir1, f)\n",
    "                   for f in os.listdir(image_dir1) if f.endswith(\".jpg\")}\n",
    "all_image_paths.update({os.path.splitext(f)[0]: os.path.join(image_dir2, f)\n",
    "                        for f in os.listdir(image_dir2) if f.endswith(\".jpg\")})\n",
    "\n",
    "df[\"path\"]  = df[\"image_id\"].map(all_image_paths)\n",
    "df = df.dropna(subset=[\"path\"]).reset_index(drop=True)\n",
    "df[\"label\"] = df[\"dx\"]\n",
    "\n",
    "assert {\"image_id\",\"lesion_id\",\"dx\",\"path\",\"label\"}.issubset(df.columns)\n",
    "print(\"Raw label counts:\\n\", df[\"label\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped split (lesion-wise) + balancing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:12:08.708040Z",
     "iopub.status.busy": "2025-08-28T06:12:08.707720Z",
     "iopub.status.idle": "2025-08-28T06:12:11.792908Z",
     "shell.execute_reply": "2025-08-28T06:12:11.791916Z",
     "shell.execute_reply.started": "2025-08-28T06:12:08.708016Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train {'akiec': 16032, 'bcc': 16032, 'bkl': 16032, 'df': 16032, 'mel': 16032, 'nv': 16032, 'vasc': 16032}\n",
      "val {'akiec': 96, 'bcc': 96, 'bkl': 96, 'df': 96, 'mel': 96, 'nv': 96, 'vasc': 96}\n",
      "test {'akiec': 93, 'bcc': 93, 'bkl': 93, 'df': 93, 'mel': 93, 'nv': 93, 'vasc': 93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/584411012.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: oversample_to_n(g, n, seed))\n",
      "/tmp/ipykernel_36/584411012.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(n=n, random_state=seed))\n",
      "/tmp/ipykernel_36/584411012.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(n=n, random_state=seed))\n"
     ]
    }
   ],
   "source": [
    "def oversample_to_n(g: pd.DataFrame, n: int, seed=SEED) -> pd.DataFrame:\n",
    "    if len(g) >= n:\n",
    "        return g.sample(n=n, random_state=seed)\n",
    "    reps = int(math.ceil(n / len(g)))\n",
    "    g_rep = pd.concat([g.sample(frac=1.0, replace=True, random_state=seed+i) for i in range(reps)], ignore_index=True)\n",
    "    return g_rep.sample(n=n, random_state=seed)\n",
    "\n",
    "def balance_split(split_df: pd.DataFrame, mode=\"down\", n_per_class=None, seed=SEED) -> pd.DataFrame:\n",
    "    counts = split_df[\"label\"].value_counts()\n",
    "    if mode == \"down\":\n",
    "        n = counts.min() if n_per_class is None else min(n_per_class, counts.min())\n",
    "        out = (split_df.groupby(\"label\", group_keys=False)\n",
    "                        .apply(lambda g: g.sample(n=n, random_state=seed))\n",
    "                        .sample(frac=1.0, random_state=seed)\n",
    "                        .reset_index(drop=True))\n",
    "    elif mode == \"over\":\n",
    "        n = counts.max() if n_per_class is None else n_per_class\n",
    "        out = (split_df.groupby(\"label\", group_keys=False)\n",
    "                        .apply(lambda g: oversample_to_n(g, n, seed))\n",
    "                        .sample(frac=1.0, random_state=seed)\n",
    "                        .reset_index(drop=True))\n",
    "    else:\n",
    "        raise ValueError(\"mode should be 'down' or 'over'\")\n",
    "    return out\n",
    "\n",
    "# encode labels\n",
    "classes_sorted = sorted(df[\"label\"].unique())\n",
    "label2id = {c:i for i,c in enumerate(classes_sorted)}\n",
    "df[\"y\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "# Stratified + Grouped split (5 folds -> 60/20/20)\n",
    "X = df[\"image_id\"].values\n",
    "y = df[\"y\"].values\n",
    "groups = df[\"lesion_id\"].values\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "folds = list(sgkf.split(X, y, groups))\n",
    "(train_idx_1, _), (train_idx_2, _), (train_idx_3, _), (val_idx, _), (test_idx, _) = folds[:5]\n",
    "\n",
    "df_train = df.iloc[np.concatenate([train_idx_1, train_idx_2, train_idx_3])].copy()\n",
    "df_val   = df.iloc[val_idx].copy()\n",
    "df_test  = df.iloc[test_idx].copy()\n",
    "\n",
    "# balance per split (same # per class as requested)\n",
    "df_train_b = balance_split(df_train, BALANCE_MODE_TRAIN, N_PER_CLASS_TRAIN)\n",
    "df_val_b   = balance_split(df_val,   BALANCE_MODE_VALTEST, N_PER_CLASS_VALTEST)\n",
    "df_test_b  = balance_split(df_test,  BALANCE_MODE_VALTEST, N_PER_CLASS_VALTEST)\n",
    "\n",
    "for name, d in [(\"train\", df_train_b), (\"val\", df_val_b), (\"test\", df_test_b)]:\n",
    "    print(name, d[\"label\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "NUM_CLASSES = len(classes_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data loaders + augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:12:17.390355Z",
     "iopub.status.busy": "2025-08-28T06:12:17.390017Z",
     "iopub.status.idle": "2025-08-28T06:12:17.877136Z",
     "shell.execute_reply": "2025-08-28T06:12:17.876316Z",
     "shell.execute_reply.started": "2025-08-28T06:12:17.390331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def df_to_ds(dff, batch=BATCH, shuffle=True):\n",
    "    paths = dff[\"path\"].values\n",
    "    labels = dff[\"y\"].values.astype(np.int32)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dff), seed=SEED, reshuffle_each_iteration=True)\n",
    "    def _load(path, y):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, IMG_SIZE)\n",
    "        return img, y\n",
    "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
    "    return ds.batch(batch).prefetch(AUTOTUNE)\n",
    "\n",
    "augment = tf.keras.Sequential([\n",
    "    L.RandomFlip(\"horizontal\"),\n",
    "    L.RandomRotation(0.05),\n",
    "    L.RandomZoom(0.08),\n",
    "    L.RandomContrast(0.08),\n",
    "])\n",
    "\n",
    "def add_aug(ds):\n",
    "    return ds.map(lambda x,y: (augment(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = add_aug(df_to_ds(df_train_b, shuffle=True))\n",
    "val_ds   = df_to_ds(df_val_b, shuffle=False)\n",
    "test_ds  = df_to_ds(df_test_b, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN backbones (MobileNetV2, ResNet50, EfficientNetB0, DenseNet121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:12:23.170718Z",
     "iopub.status.busy": "2025-08-28T06:12:23.169920Z",
     "iopub.status.idle": "2025-08-28T06:12:23.181688Z",
     "shell.execute_reply": "2025-08-28T06:12:23.180576Z",
     "shell.execute_reply.started": "2025-08-28T06:12:23.170686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_cnn_backbone(name):\n",
    "    name = name.lower()\n",
    "    if name == \"mobilenetv2\":\n",
    "        base = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=IMG_SIZE+(3,))\n",
    "        preprocess = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "        last_conv = \"Conv_1\"\n",
    "    elif name == \"resnet50\":\n",
    "        base = tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\", input_shape=IMG_SIZE+(3,))\n",
    "        preprocess = tf.keras.applications.resnet.preprocess_input\n",
    "        last_conv = \"conv5_block3_out\"\n",
    "    elif name == \"efficientnetb0\":\n",
    "        base = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=IMG_SIZE+(3,))\n",
    "        preprocess = tf.keras.applications.efficientnet.preprocess_input\n",
    "        last_conv = \"top_conv\"\n",
    "    elif name == \"densenet121\":\n",
    "        base = tf.keras.applications.DenseNet121(include_top=False, weights=\"imagenet\", input_shape=IMG_SIZE+(3,))\n",
    "        preprocess = tf.keras.applications.densenet.preprocess_input\n",
    "        last_conv = None  # we'll auto-find last conv layer\n",
    "    else:\n",
    "        raise ValueError(\"Unknown CNN name\")\n",
    "    return base, preprocess, last_conv\n",
    "\n",
    "def find_last_conv_layer(model):\n",
    "    for layer in reversed(model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            return layer.name\n",
    "    return None\n",
    "\n",
    "def classifier_head_2d(x, num_classes):\n",
    "    x = L.GlobalAveragePooling2D()(x)\n",
    "    x = L.Dropout(0.35)(x)\n",
    "    return L.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "def make_cnn_model(name, num_classes=NUM_CLASSES):\n",
    "    base, preprocess, last_conv = build_cnn_backbone(name)\n",
    "    inp = L.Input(shape=IMG_SIZE+(3,))\n",
    "    x = preprocess(inp)\n",
    "    feat = base(x, training=False)\n",
    "    out = classifier_head_2d(feat, num_classes)\n",
    "    model = Model(inp, out, name=name)\n",
    "    model.base = base\n",
    "    model.preprocess = preprocess\n",
    "    model.last_conv_name = last_conv or find_last_conv_layer(base)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny ViT with attention-rollout helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:12:28.359088Z",
     "iopub.status.busy": "2025-08-28T06:12:28.358366Z",
     "iopub.status.idle": "2025-08-28T06:12:28.372099Z",
     "shell.execute_reply": "2025-08-28T06:12:28.371043Z",
     "shell.execute_reply.started": "2025-08-28T06:12:28.359056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def patch_embed(img, patch=16, dim=384):\n",
    "    x = L.Conv2D(dim, kernel_size=patch, strides=patch, padding=\"valid\", name=\"patch_embed\")(img)\n",
    "    x = L.Reshape((-1, dim))(x)  # (B, N, D)\n",
    "    return x\n",
    "\n",
    "def transformer_block(x, heads=6, dim=384, mlp_dim=768, drop=0.0, name=\"blk\"):\n",
    "    ln1 = L.LayerNormalization(name=name+\"_ln1\")(x)\n",
    "    mha  = L.MultiHeadAttention(num_heads=heads, key_dim=dim//heads, dropout=drop, name=name+\"_mha\")\n",
    "    attn_out, attn_scores = mha(ln1, ln1, return_attention_scores=True)\n",
    "    x = L.Add(name=name+\"_add1\")([x, L.Dropout(drop)(attn_out)])\n",
    "    ln2 = L.LayerNormalization(name=name+\"_ln2\")(x)\n",
    "    h = L.Dense(mlp_dim, activation=\"gelu\", name=name+\"_mlp1\")(ln2)\n",
    "    h = L.Dropout(drop)(h)\n",
    "    h = L.Dense(dim, name=name+\"_mlp2\")(h)\n",
    "    x = L.Add(name=name+\"_add2\")([x, L.Dropout(drop)(h)])\n",
    "    return x, attn_scores\n",
    "\n",
    "def make_vit_tiny(num_classes=NUM_CLASSES, patch=16, dim=384, depth=6, heads=6, mlp_dim=768, drop=0.0):\n",
    "    inp = L.Input(shape=IMG_SIZE+(3,), name=\"vit_input\")\n",
    "    x = (inp - 127.5) / 127.5\n",
    "    tok = patch_embed(x, patch, dim)\n",
    "\n",
    "    # CLS token via embedding of a zero index\n",
    "    B_tokens = L.Lambda(lambda t: tf.shape(t)[1])(tok)\n",
    "    num_tokens = (IMG_SIZE[0]//patch)*(IMG_SIZE[1]//patch) + 1\n",
    "    cls_idx = L.Lambda(lambda t: tf.zeros((tf.shape(t)[0], 1), dtype=tf.int32))(tok)\n",
    "    cls_tok = L.Embedding(1, dim, name=\"cls_token\")(cls_idx)\n",
    "    tok = L.Concatenate(axis=1)([cls_tok, tok])\n",
    "\n",
    "    pos = L.Embedding(input_dim=num_tokens, output_dim=dim, name=\"pos_embed\")(tf.range(0, num_tokens))\n",
    "    tok = tok + pos  # broadcast over batch\n",
    "\n",
    "    attn_tensors = []\n",
    "    for i in range(depth):\n",
    "        tok, attn = transformer_block(tok, heads, dim, mlp_dim, drop, name=f\"blk{i+1}\")\n",
    "        attn_tensors.append(attn)\n",
    "\n",
    "    cls = L.Lambda(lambda t: t[:,0], name=\"cls_slice\")(tok)\n",
    "    penult = L.Dropout(drop, name=\"pre_logits\")(cls)\n",
    "    out = L.Dense(num_classes, activation=\"softmax\", name=\"head\")(penult)\n",
    "\n",
    "    model = Model(inp, out, name=\"vit_tiny\")\n",
    "    attn_model = Model(inp, attn_tensors, name=\"vit_tiny_attn\")\n",
    "    feat_model = Model(inp, penult, name=\"vit_tiny_feat\")\n",
    "\n",
    "    model.preprocess = lambda z: (z - 127.5) / 127.5\n",
    "    return model, attn_model, feat_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval, attention, and overlay saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T09:27:08.333632Z",
     "iopub.status.busy": "2025-08-28T09:27:08.333308Z",
     "iopub.status.idle": "2025-08-28T09:27:08.345370Z",
     "shell.execute_reply": "2025-08-28T09:27:08.344431Z",
     "shell.execute_reply.started": "2025-08-28T09:27:08.333593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_ds, val_ds, epochs_head=EPOCHS_HEAD, epochs_fine=EPOCHS_FINE, unfreeze_ratio=0.4):\n",
    "    # ----- Phase 1: train the head -----\n",
    "    if hasattr(model, \"base\"):\n",
    "        model.base.trainable = False\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    es = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=epochs_head, callbacks=[es], verbose=1)\n",
    "\n",
    "    # ----- Phase 2: fine-tune last X% of backbone -----\n",
    "    if hasattr(model, \"base\"):\n",
    "        model.base.trainable = True\n",
    "        if hasattr(model.base, \"layers\"):\n",
    "            k = int((1.0 - unfreeze_ratio) * len(model.base.layers))\n",
    "            for i, layer in enumerate(model.base.layers):\n",
    "                layer.trainable = (i >= k)\n",
    "\n",
    "    # ü©π label_smoothing fallback for older TF\n",
    "    try:\n",
    "        loss_fine = tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1)\n",
    "    except TypeError:\n",
    "        loss_fine = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "        loss=loss_fine,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=epochs_fine, callbacks=[es], verbose=1)\n",
    "\n",
    "    # ----- Validation eval -----\n",
    "    y_true, y_prob = [], []\n",
    "    for xb, yb in val_ds:\n",
    "        y_true.append(yb.numpy())\n",
    "        y_prob.append(model.predict(xb, verbose=0))\n",
    "    y_true = np.concatenate(y_true); y_prob = np.concatenate(y_prob)\n",
    "    y_pred = y_prob.argmax(axis=1)\n",
    "\n",
    "    acc = (y_pred == y_true).mean()\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    try:\n",
    "        auroc_macro = roc_auc_score(tf.one_hot(y_true, NUM_CLASSES).numpy(), y_prob, average=\"macro\", multi_class=\"ovr\")\n",
    "    except Exception:\n",
    "        auroc_macro = float(\"nan\")\n",
    "    report = classification_report(y_true, y_pred, target_names=classes_sorted, digits=4)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return dict(acc=acc, bal_acc=bal_acc, f1_macro=f1_macro, auroc_macro=auroc_macro,\n",
    "                report=report, cm=cm, y_true=y_true, y_prob=y_prob, y_pred=y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_cnn_model(\"mobilenetv2\", NUM_CLASSES)\n",
    "tmp_metrics = train_and_eval(tmp_model, train_ds, val_ds, epochs_head=EPOCHS_HEAD, epochs_fine=EPOCHS_FINE, unfreeze_ratio=0.5)\n",
    "print({k: v if not isinstance(v, np.ndarray) else '...' for k,v in tmp_metrics.items() if k in [\"acc\",\"bal_acc\",\"f1_macro\",\"auroc_macro\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_cnn_model(\"resnet50\", NUM_CLASSES)\n",
    "tmp_metrics = train_and_eval(tmp_model, train_ds, val_ds, epochs_head=EPOCHS_HEAD, epochs_fine=EPOCHS_FINE, unfreeze_ratio=0.5)\n",
    "print({k: v if not isinstance(v, np.ndarray) else '...' for k,v in tmp_metrics.items() if k in [\"acc\",\"bal_acc\",\"f1_macro\",\"auroc_macro\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_cnn_model(\"efficientnetb0\", NUM_CLASSES)\n",
    "tmp_metrics = train_and_eval(tmp_model, train_ds, val_ds, epochs_head=EPOCHS_HEAD, epochs_fine=EPOCHS_FINE, unfreeze_ratio=0.5)\n",
    "print({k: v if not isinstance(v, np.ndarray) else '...' for k,v in tmp_metrics.items() if k in [\"acc\",\"bal_acc\",\"f1_macro\",\"auroc_macro\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all models + save attention previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name in MODELS_TO_RUN:\n",
    "    print(f\"\\n========== Training {name} ==========\")\n",
    "    if name == \"vit_tiny\":\n",
    "        vit_model, vit_attn_model, vit_feat_model = make_vit_tiny(NUM_CLASSES, patch=16, dim=384, depth=6, heads=6, drop=0.0)\n",
    "        model = vit_model\n",
    "        metrics = train_and_eval(model, train_ds, val_ds)\n",
    "        save_attention_overlays(name, model, val_ds, vit_attn_model, out_dir=OUTPUT_DIR)\n",
    "        results[name] = {**metrics, \"feat_model\": vit_feat_model, \"attn_model\": vit_attn_model, \"model\": model}\n",
    "    else:\n",
    "        model = make_cnn_model(name, NUM_CLASSES)\n",
    "        metrics = train_and_eval(model, train_ds, val_ds)\n",
    "        save_attention_overlays(name, model, val_ds, out_dir=OUTPUT_DIR)\n",
    "        results[name] = {**metrics, \"model\": model}\n",
    "\n",
    "print(\"\\n=== Validation Results ===\")\n",
    "print(\"Model\\tAcc\\tBalAcc\\tMacroF1\\tMacroAUROC\")\n",
    "for k,v in results.items():\n",
    "    au = v['auroc_macro'] if v['auroc_macro'] == v['auroc_macro'] else float('nan')\n",
    "    print(f\"{k}\\t{v['acc']*100:.1f}\\t{v['bal_acc']*100:.1f}\\t{v['f1_macro']:.3f}\\t{au:.3f}\")\n",
    "\n",
    "# save textual reports & confusion matrices\n",
    "for k,v in results.items():\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{k}_VAL_report.txt\"), \"w\") as f:\n",
    "        f.write(v[\"report\"])\n",
    "    np.save(os.path.join(OUTPUT_DIR, f\"{k}_VAL_cm.npy\"), v[\"cm\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft-vote ensemble on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def soft_vote(probs_list, weights=None):\n",
    "    P = np.stack(probs_list, axis=0)\n",
    "    if weights is None:\n",
    "        weights = np.ones((len(probs_list), 1, 1))\n",
    "    else:\n",
    "        weights = np.asarray(weights)[:, None, None]\n",
    "    P = (P * weights).sum(axis=0) / weights.sum()\n",
    "    return P\n",
    "\n",
    "# choose top-3 by macro-F1\n",
    "top3 = sorted(results.items(), key=lambda kv: kv[1][\"f1_macro\"], reverse=True)[:3]\n",
    "ens_probs = soft_vote([kv[1][\"y_prob\"] for kv in top3])\n",
    "y_true_val = top3[0][1][\"y_true\"]\n",
    "ens_pred = ens_probs.argmax(axis=1)\n",
    "\n",
    "ens_acc = (ens_pred == y_true_val).mean()\n",
    "ens_bal = balanced_accuracy_score(y_true_val, ens_pred)\n",
    "ens_f1  = f1_score(y_true_val, ens_pred, average=\"macro\")\n",
    "ens_auroc = roc_auc_score(tf.one_hot(y_true_val, NUM_CLASSES).numpy(), ens_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "print(f\"Ensemble (top-3 soft vote) ‚Äî Acc: {ens_acc:.3f}, BalAcc: {ens_bal:.3f}, MacroF1: {ens_f1:.3f}, MacroAUROC: {ens_auroc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid CNN+ViT (feature fusion) and validation eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_hybrid_cnn_vit(cnn_model_name=\"efficientnetb0\", vit_feat_model=None, num_classes=NUM_CLASSES):\n",
    "    # CNN branch\n",
    "    base, preprocess, _ = build_cnn_backbone(cnn_model_name)\n",
    "    inpx = L.Input(shape=IMG_SIZE+(3,))\n",
    "    x_cnn = preprocess(inpx)\n",
    "    feat_cnn = base(x_cnn, training=False)\n",
    "    feat_cnn = L.GlobalAveragePooling2D()(feat_cnn)\n",
    "\n",
    "    # ViT feature branch\n",
    "    if vit_feat_model is None:\n",
    "        vit_model, vit_attn_model, vit_feat_model = make_vit_tiny(NUM_CLASSES)\n",
    "    feat_vit = vit_feat_model((inpx - 127.5)/127.5)\n",
    "\n",
    "    fused = L.Concatenate()([feat_cnn, feat_vit])\n",
    "    fused = L.Dropout(0.4)(fused)\n",
    "    fused = L.Dense(512, activation=\"gelu\")(fused)\n",
    "    out = L.Dense(num_classes, activation=\"softmax\")(fused)\n",
    "\n",
    "    model = Model(inpx, out, name=f\"hybrid_{cnn_model_name}_vit\")\n",
    "    model.base_cnn = base\n",
    "    model.base_vit = vit_feat_model\n",
    "    return model\n",
    "\n",
    "vit_feat = results[\"vit_tiny\"][\"feat_model\"]\n",
    "hybrid = make_hybrid_cnn_vit(\"efficientnetb0\", vit_feat_model=vit_feat)\n",
    "hyb_val = train_and_eval(hybrid, train_ds, val_ds, unfreeze_ratio=0.3)\n",
    "\n",
    "print(\"Hybrid (VAL) ‚Äî Acc: {:.3f}, BalAcc: {:.3f}, MacroF1: {:.3f}, MacroAUROC: {:.3f}\".format(\n",
    "    hyb_val[\"acc\"], hyb_val[\"bal_acc\"], hyb_val[\"f1_macro\"], hyb_val[\"auroc_macro\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final TEST evaluation (best single, ensemble, hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def eval_on_test(model, test_ds):\n",
    "    y_true, y_prob = [], []\n",
    "    for xb, yb in test_ds:\n",
    "        y_true.append(yb.numpy()); y_prob.append(model.predict(xb, verbose=0))\n",
    "    y_true = np.concatenate(y_true); y_prob = np.concatenate(y_prob); y_pred = y_prob.argmax(axis=1)\n",
    "    return dict(\n",
    "        acc=(y_pred==y_true).mean(),\n",
    "        bal_acc=balanced_accuracy_score(y_true, y_pred),\n",
    "        f1_macro=f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        auroc_macro=roc_auc_score(tf.one_hot(y_true, NUM_CLASSES).numpy(), y_prob, average=\"macro\", multi_class=\"ovr\"),\n",
    "        report=classification_report(y_true, y_pred, target_names=classes_sorted, digits=4),\n",
    "        cm=confusion_matrix(y_true, y_pred)\n",
    "    )\n",
    "\n",
    "best_single = max(results.items(), key=lambda kv: kv[1][\"f1_macro\"])[0]\n",
    "print(f\"Best single on VAL by Macro-F1: {best_single}\")\n",
    "\n",
    "# (a) best single model on TEST\n",
    "test_single = eval_on_test(results[best_single][\"model\"], test_ds)\n",
    "\n",
    "# (b) ensemble on TEST (recompute per batch to save memory)\n",
    "top3 = sorted(results.items(), key=lambda kv: kv[1][\"f1_macro\"], reverse=True)[:3]\n",
    "y_true_test, probs_each = [], []\n",
    "for xb, yb in test_ds:\n",
    "    y_true_test.append(yb.numpy())\n",
    "    probs_each.append(soft_vote([r[\"model\"].predict(xb, verbose=0) for _, r in top3]))\n",
    "y_true_test = np.concatenate(y_true_test); probs_ens = np.concatenate(probs_each)\n",
    "y_pred_ens = probs_ens.argmax(axis=1)\n",
    "test_ens = dict(\n",
    "    acc=(y_pred_ens==y_true_test).mean(),\n",
    "    bal_acc=balanced_accuracy_score(y_true_test, y_pred_ens),\n",
    "    f1_macro=f1_score(y_true_test, y_pred_ens, average=\"macro\"),\n",
    "    auroc_macro=roc_auc_score(tf.one_hot(y_true_test, NUM_CLASSES).numpy(), probs_ens, average=\"macro\", multi_class=\"ovr\")\n",
    ")\n",
    "\n",
    "# (c) hybrid on TEST\n",
    "test_hybrid = eval_on_test(hybrid, test_ds)\n",
    "\n",
    "print(\"\\n=== TEST Results ===\")\n",
    "print(\"Single(best)\\tAcc\\tBalAcc\\tMacroF1\\tMacroAUROC\")\n",
    "print(f\"{best_single}\\t{test_single['acc']*100:.1f}\\t{test_single['bal_acc']*100:.1f}\\t{test_single['f1_macro']:.3f}\\t{test_single['auroc_macro']:.3f}\")\n",
    "\n",
    "print(\"\\nEnsemble(top-3)\\tAcc\\tBalAcc\\tMacroF1\\tMacroAUROC\")\n",
    "print(f\"ensemble\\t{test_ens['acc']*100:.1f}\\t{test_ens['bal_acc']*100:.1f}\\t{test_ens['f1_macro']:.3f}\\t{test_ens['auroc_macro']:.3f}\")\n",
    "\n",
    "print(\"\\nHybrid(EffB0+ViT)\\tAcc\\tBalAcc\\tMacroF1\\tMacroAUROC\")\n",
    "print(f\"hybrid\\t{test_hybrid['acc']*100:.1f}\\t{test_hybrid['bal_acc']*100:.1f}\\t{test_hybrid['f1_macro']:.3f}\\t{test_hybrid['auroc_macro']:.3f}\")\n",
    "\n",
    "# save reports\n",
    "with open(os.path.join(OUTPUT_DIR, f\"{best_single}_TEST_report.txt\"), \"w\") as f: f.write(test_single[\"report\"])\n",
    "with open(os.path.join(OUTPUT_DIR, \"ensemble_TEST_metrics.json\"), \"w\") as f: json.dump({k: float(v) for k,v in test_ens.items()}, f, indent=2)\n",
    "with open(os.path.join(OUTPUT_DIR, \"hybrid_TEST_report.txt\"), \"w\") as f: f.write(test_hybrid[\"report\"])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
